---
title: RAG Assistant - Apple Organization
emoji: ğŸš€
colorFrom: blue
colorTo: purple
sdk: streamlit
sdk_version: "1.28.1"
app_file: app.py
pinned: false
---

# ğŸš€ RAG Assistant - Apple Organization Analysis

A **Retrieval-Augmented Generation (RAG)** application that answers questions about Apple's organizational structure and innovation processes using multiple LLMs.

## ğŸ“‹ Features

- ğŸ¯ **Multi-LLM Support**: Ollama (local), OpenAI, Google Gemini, Claude
- ğŸ“„ **PDF-based Knowledge Base**: Automatically processes and indexes documents
- ğŸ” **Semantic Search**: Fast similarity-based retrieval using FAISS
- ğŸ’» **Beautiful UI**: Clean Streamlit interface
- ğŸ”’ **No API Keys Stored**: Secure API key input via UI
- ğŸ“Š **Evaluation Metrics**: Retrieval quality & answer similarity tracking

... rest of your README

## ğŸ¯ What Can It Do?

Ask questions about:
- Apple's organizational structure
- Functional organization benefits
- Leadership model and characteristics
- Innovation processes
- Cross-functional collaboration
- And more!

## ğŸš€ Quick Start (Local)

### Prerequisites
- Python 3.8+
- Ollama (for local LLM) - [Install here](https://ollama.ai)

### Installation
```bash
# Clone repository
git clone <your-repo-url>
cd rag-case-study

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download Ollama model (if using local LLM)
ollama pull llama3.2:1b

# Start Ollama
ollama serve
```

### Running the App
```bash
# In a new terminal, from project root
streamlit run app.py
```

Open your browser at `http://localhost:8501`

## ğŸ“ Usage

1. **Select LLM Provider** (sidebar):
   - **Ollama (Local)**: No API key needed
   - **OpenAI**: Paste your API key
   - **Google Gemini**: Paste your API key
   - **Claude**: Paste your API key

2. **Ask a Question**:
   - Type your question about Apple's organization
   - Click "ğŸ” Search"

3. **Review Results**:
   - See retrieved relevant documents
   - Read the AI-generated answer
   - Check similarity scores

## ğŸ—ï¸ Project Structure
```
rag-case-study/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ config.yaml                 # Configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ HBR_How_Apple_Is_Organized_For_Innovation-4.pdf
â”‚   â”œâ”€â”€ extracted_text.txt
â”‚   â”œâ”€â”€ chunks.json
â”‚   â”œâ”€â”€ embeddings.json
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_pdf.py          # Extract text from PDF
â”‚   â”œâ”€â”€ chunk_text.py           # Split text into chunks
â”‚   â”œâ”€â”€ generate_embeddings.py  # Create embeddings
â”‚   â”œâ”€â”€ build_vector_db.py      # Build FAISS index
â”‚   â”œâ”€â”€ evaluate_rag.py         # Evaluate system performance
â”‚   â””â”€â”€ test_dataset.json       # Test questions
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ retriever.py            # Semantic search
    â”œâ”€â”€ llm_handler.py          # LLM integration
    â””â”€â”€ rag_chain.py            # Complete RAG pipeline
```

## ğŸ”§ Configuration

Edit `config.yaml` to customize:
```yaml
# Chunk settings
chunking:
  chunk_size: 500        # Characters per chunk
  chunk_overlap: 50      # Overlap between chunks

# Retrieval settings
retrieval:
  top_k: 5              # Number of results to retrieve

# LLM settings
llm:
  ollama:
    model: "llama3.2:1b"
    temperature: 0.7
```

## ğŸ“Š Evaluation

Run comprehensive RAG evaluation:
```bash
python scripts/evaluate_rag.py
```

This generates metrics:
- **Retrieval**: Precision@5, Recall@5, MRR
- **Answer Quality**: Semantic similarity scores
- **Report**: Saved as `evaluation_report.json`

## ğŸ“ How It Works

### RAG Pipeline:

1. **Document Processing**
   - Extract text from PDF
   - Split into overlapping chunks
   - Generate embeddings using `sentence-transformers`

2. **Vector Storage**
   - Store embeddings in FAISS index
   - Fast similarity search

3. **Query Processing**
   - User asks a question
   - Convert query to embedding
   - Search FAISS for similar chunks (top-5)

4. **Answer Generation**
   - Send retrieved chunks + question to LLM
   - LLM generates contextual answer
   - Return answer with source attribution

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Streamlit |
| **RAG Framework** | LangChain |
| **Embeddings** | Sentence-Transformers |
| **Vector DB** | FAISS |
| **LLMs** | Ollama, OpenAI, Gemini, Claude |
| **Config** | YAML |

## ğŸ” Security

- âœ… API keys **not stored** in code
- âœ… API keys **not committed** to git
- âœ… Secure input masking in UI
- âœ… `.gitignore` excludes sensitive files

## ğŸ“ˆ Next Steps (Version 2)

- [ ] Multiple PDF support
- [ ] Query history & favorites
- [ ] User feedback mechanism
- [ ] Answer source attribution
- [ ] Advanced controls (top_k slider, temperature)
- [ ] Dashboard with analytics

## ğŸ¤ Contributing

Feedback and suggestions welcome! This is Version 1 (Pilot).

## ğŸ“„ License

MIT License

## ğŸ‘¨â€ğŸ’» Author

[Your Name]

## ğŸ“§ Support

For issues or questions, please open an issue on GitHub.

---

**Made with â¤ï¸ for learning RAG systems**